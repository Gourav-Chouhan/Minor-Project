{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "import preprocessor as p\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('idk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rwanda is set to host the headquarters of united nations development programmes undp new innovation financing facilityfind out more rwanda'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "embedding_dim = 32\n",
    "max_length = 300\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 20000\n",
    "training_sentences = df['Text'][0:training_size].to_list()\n",
    "testing_sentences = df['Text'][training_size:].to_list()\n",
    "training_labels = df['Label'][0:training_size].to_list()\n",
    "testing_labels = df['Label'][training_size:].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.text.Tokenizer at 0x7f04742af550>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need this block to get it to work with TensorFlow 2.x\n",
    "import numpy as np\n",
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 300, 32)           640000    \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 9600)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               1228928   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,869,057\n",
      "Trainable params: 1,869,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "625/625 [==============================] - 17s 26ms/step - loss: 0.2669 - accuracy: 0.8705 - val_loss: 0.0966 - val_accuracy: 0.9624\n",
      "Epoch 2/4\n",
      "625/625 [==============================] - 16s 26ms/step - loss: 0.0509 - accuracy: 0.9823 - val_loss: 0.1025 - val_accuracy: 0.9598\n",
      "Epoch 3/4\n",
      "625/625 [==============================] - 17s 27ms/step - loss: 0.0227 - accuracy: 0.9933 - val_loss: 0.1252 - val_accuracy: 0.9549\n",
      "Epoch 4/4\n",
      "625/625 [==============================] - 16s 25ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.1332 - val_accuracy: 0.9551\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 4\n",
    "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:03:47\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "print(current_time)\n",
    "\n",
    "model.save(f\"./saved_models/simple_ann({current_time}).h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 300, 32)           640000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 296, 128)          20608     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 660,737\n",
      "Trainable params: 660,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import  Sequential\n",
    "from keras.layers import Embedding, LSTM, Bidirectional, Conv1D, GlobalMaxPooling1D, Dense\n",
    "cnn_model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(cnn_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 14s 21ms/step - loss: 0.1698 - acc: 0.9226 - val_loss: 0.0723 - val_acc: 0.9691\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 13s 22ms/step - loss: 0.0430 - acc: 0.9861 - val_loss: 0.0778 - val_acc: 0.9680\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0229 - acc: 0.9932 - val_loss: 0.0895 - val_acc: 0.9691\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0120 - acc: 0.9966 - val_loss: 0.1036 - val_acc: 0.9648\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0072 - acc: 0.9982 - val_loss: 0.1114 - val_acc: 0.9657\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 0.0051 - acc: 0.9985 - val_loss: 0.1192 - val_acc: 0.9645\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 0.0045 - acc: 0.9987 - val_loss: 0.1273 - val_acc: 0.9638\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 0.0044 - acc: 0.9987 - val_loss: 0.1327 - val_acc: 0.9630\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.1433 - val_acc: 0.9613\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 15s 24ms/step - loss: 0.0037 - acc: 0.9991 - val_loss: 0.1394 - val_acc: 0.9635\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history_cnn = cnn_model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lstm_model = tf.keras.Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    Bidirectional(LSTM(128)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 300, 32)           640000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 256)              164864    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 805,121\n",
      "Trainable params: 805,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 10\n",
    "# lstm_model_histroy = lstm_model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(s):\n",
    "    sequences = tokenizer.texts_to_sequences([s])\n",
    "    padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "    # x = 0\n",
    "    # if model_name == 'lstm':\n",
    "    y = cnn_model.predict(padded)\n",
    "    # elif model_name == 'ann':\n",
    "    x = model.predict(padded)\n",
    "      \n",
    "    return f\"ANN: {x}, CNN: {y}\"\n",
    "    # if x > 0.7:\n",
    "    #     return f\"Positive: confidence {x}\" \n",
    "    # elif x < 0.33:\n",
    "    #     return f\"Negative: confidence {x}\"\n",
    "    # return f\"Can't say: confidence {x}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ANN: [[0.10401616]], CNN: [[0.05187383]]'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction(\"i am having a mental breakdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflowjs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflowjs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfjs\u001b[39;00m\n\u001b[1;32m      2\u001b[0m tfjs\u001b[38;5;241m.\u001b[39mconverters\u001b[38;5;241m.\u001b[39msave_keras_model(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./folder\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflowjs'"
     ]
    }
   ],
   "source": [
    "import tensorflowjs as tfjs\n",
    "tfjs.converters.save_keras_model(model, './folder')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
